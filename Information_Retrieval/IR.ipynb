{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading requirments ... \n",
      "loading main data ... \n",
      "loading clustring data ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirhossein/anaconda3/envs/AI/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator KMeans from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_model =  KMeans(algorithm='auto', max_iter=1000, n_clusters=12)\n",
      "loading Boolean search model\n",
      "boolean search loading modules\n",
      "boolean search loading authors preprocess\n",
      "boolean search loading title preprocess\n",
      "loading tf-idf search model\n",
      "loading tf-idf model modules\n",
      "loading fasttext module\n",
      "loading fasttext requirments\n",
      "process fasttext module\n",
      "Transformers loading\n",
      "Transformer\n",
      "downloading model all-MiniLM-L12-v2\n",
      "page_ranking_algorithm loading\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 394>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=384'>385</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbest_articles\u001b[39m(\u001b[39mself\u001b[39m,type_query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpage_rank\u001b[39m\u001b[39m\"\u001b[39m,mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m,k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m):\u001b[39m# page_rank,hits\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=385'>386</a>\u001b[0m         \u001b[39m# type_query = page_rank , hits ,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=386'>387</a>\u001b[0m         \u001b[39m# k = numbers\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=387'>388</a>\u001b[0m         \u001b[39m# mode = artcile , author\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=389'>390</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=393'>394</a>\u001b[0m ir \u001b[39m=\u001b[39m IR()\n",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36mIR.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=352'>353</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpage_ranking_algorithm loading\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=353'>354</a>\u001b[0m page_hits_articles \u001b[39m=\u001b[39m Page_Ranking_Hits()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=354'>355</a>\u001b[0m page_hits_articles\u001b[39m.\u001b[39;49mcompute_page_rank(\u001b[39m0.9\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=355'>356</a>\u001b[0m page_hits_articles\u001b[39m.\u001b[39mcompute_hits()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=358'>359</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished loading packages.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36mPage_Ranking_Hits.compute_page_rank\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=294'>295</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_page_rank\u001b[39m(\u001b[39mself\u001b[39m, alpha \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=295'>296</a>\u001b[0m     graph \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mfrom_numpy_array(A\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mtoarray(), create_using\u001b[39m=\u001b[39mnx\u001b[39m.\u001b[39mDiGraph)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=296'>297</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpr \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mpagerank(G\u001b[39m=\u001b[39;49mgraph, alpha\u001b[39m=\u001b[39;49malpha)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:108\u001b[0m, in \u001b[0;36mpagerank\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpagerank\u001b[39m(\n\u001b[1;32m     10\u001b[0m     G,\n\u001b[1;32m     11\u001b[0m     alpha\u001b[39m=\u001b[39m\u001b[39m0.85\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     dangling\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m ):\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\"\"Returns the PageRank of the nodes in the graph.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[39m    PageRank computes a ranking of the nodes in the graph G based on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m pagerank_scipy(\n\u001b[1;32m    109\u001b[0m         G, alpha, personalization, max_iter, tol, nstart, weight, dangling\n\u001b[1;32m    110\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:469\u001b[0m, in \u001b[0;36mpagerank_scipy\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[1;32m    468\u001b[0m nodelist \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(G)\n\u001b[0;32m--> 469\u001b[0m A \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mto_scipy_sparse_array(G, nodelist\u001b[39m=\u001b[39;49mnodelist, weight\u001b[39m=\u001b[39;49mweight, dtype\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m)\n\u001b[1;32m    470\u001b[0m S \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    471\u001b[0m S[S \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m S[S \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/networkx/convert_matrix.py:909\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[0;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[1;32m    906\u001b[0m     row, col, data \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    908\u001b[0m \u001b[39mif\u001b[39;00m G\u001b[39m.\u001b[39mis_directed():\n\u001b[0;32m--> 909\u001b[0m     A \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mcoo_array((data, (row, col)), shape\u001b[39m=\u001b[39m(nlen, nlen), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    910\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    911\u001b[0m     \u001b[39m# symmetrize matrix\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     d \u001b[39m=\u001b[39m data \u001b[39m+\u001b[39m data\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    }
   ],
   "source": [
    "# %%writefile mir.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enum\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "# nltk.download()\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "# pip install -U sentence-transformers\n",
    "source_path = \"./\"\n",
    "def address_resolver(add):\n",
    "    return source_path + add\n",
    "\n",
    "def flatten(l : List[List]) -> List:\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "    ABSTRACT = \"Abstract Based\"\n",
    "class Boolean_IR:\n",
    "    def __init__(self,docs):\n",
    "        print(\"boolean search loading modules\")\n",
    "        self.author_to_id = json.load(open(\"DATA/P3/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/P3/author_to_doc.json\",\"r\"))\n",
    "        self.documents = docs\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/P3/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def word_tokenize_author(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        print(\"boolean search loading authors preprocess\")\n",
    "        self.all_names = list(set(flatten([self.word_tokenize_author(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_author(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        print(\"boolean search loading title preprocess\")\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "            \n",
    "    def title_ir(self,wk:str , k):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)\n",
    "            articles = [self.documents[id[0]] for id in mapping]\n",
    "            return (articles,mapping)\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_authoe(input_string.strip()),k) \n",
    "            articles = flatten([[self.documents[id] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])\n",
    "            return (articles[k[0]:k[1]],names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TF_IDF_IR:\n",
    "    def __init__(self,docs):\n",
    "\n",
    "        self.documents = docs\n",
    "        print(\"loading tf-idf model modules\")\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.lemma_abs = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "        self.idf_abs = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        self.idf_title = json.load(open(\"DATA/P3/idf_title.json\",\"r\"))\n",
    "        self.tf_title = json.load(open(\"DATA/P3/title_tf.json\",\"r\"))\n",
    "        self.tf_abs = json.load(open(\"DATA/P3/asb_tf.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "        for key in self.tf_title:\n",
    "            self.tf_title[key] = {int(k) : float(self.tf_title[key][k]) for k in self.tf_title[key]}\n",
    "        for key in self.tf_abs:\n",
    "            self.tf_abs[key] = {int(k) : float(self.tf_abs[key][k]) for k in self.tf_abs[key]}\n",
    "        self.lemma_title = {key : int(self.lemma_title[key]) for key in self.lemma_title}\n",
    "        self.lemma_abs = {key : int(self.lemma_abs[key]) for key in self.lemma_abs}\n",
    "        self.idf_abs =  {int(key) : float(self.idf_abs[key]) for key in self.idf_abs}\n",
    "        self.idf_title =  {int(key) : float(self.idf_title[key]) for key in self.idf_title}\n",
    "\n",
    "    def process_q(self,q : List , tf , idf , k) -> List[Tuple]:\n",
    "        return sorted([(key,sum([tf[key].get(wq,0) * idf.get(wq,0) for wq in q])) for key in tf], key = lambda x : x[1] , reverse=True)[:k]\n",
    "        \n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k : int = 10) -> List:\n",
    "        wk = self.tokenizer(input_string.strip().lower())\n",
    "        if type == Query_type.TITLE:\n",
    "            q = [int(self.lemma_title.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_title,self.idf_title , k)\n",
    "        elif type == Query_type.ABSTRACT:\n",
    "            q = [int(self.lemma_abs.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_abs,self.idf_abs , k)\n",
    "        articles = [self.documents[id[0]] for id in result]\n",
    "        return (articles,result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,docs,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = docs\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        print(\"loading fasttext requirments\")\n",
    "        if t == \"lemma\":  \n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            print(\"training fasttext module\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            print(\"\\n making fastext\")\n",
    "            os.chdir('./fasttext/fastText')\n",
    "            os.system(\"make\")\n",
    "            os.chdir('../..')\n",
    "            print(os.getcwd())\n",
    "            print(\"./fasttext/fastText/fasttext module\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array) -> List[Tuple]:\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        \n",
    "\n",
    "    def query(self, input_string:str , k : int = 10) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q)[:k]\n",
    "        articles = [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,articles)\n",
    "\n",
    "source = \"./\"\n",
    "f_source = lambda s : source+\"/\"+s\n",
    "class Transformer:\n",
    "  def __init__(self,docs,model_name = './DATA/sentence-transformers_all-MiniLM-L12-v2/'):\n",
    "    print(f\"Transformer\\ndownloading model {model_name}\")\n",
    "    self.model = SentenceTransformer(model_name)\n",
    "    self.documents = docs\n",
    "    self.representation = None\n",
    "  \n",
    "  def preprocess(self,pre_use = False):\n",
    "    if not pre_use:\n",
    "      docs = []\n",
    "      keys = []\n",
    "      print(f\"creating representation for docs\")\n",
    "      for key in self.documents:\n",
    "        abstract = self.documents[key][\"abstract\"]\n",
    "        if type(abstract) == str:\n",
    "          docs.append(abstract)\n",
    "          keys.append(key)\n",
    "      embeddings = self.model.encode(docs)\n",
    "      self.representation = {}\n",
    "      for key, embedding in zip(keys, embeddings):\n",
    "        self.representation[key] = embedding.tolist()\n",
    "      addr = f_source(\"DATA/P3/transformer.json\")\n",
    "      print(f\"saving docs_rep in {addr}\")\n",
    "      open(addr,\"w\").write(json.dumps(self.representation))\n",
    "    print(f\"loading docs_rep\")\n",
    "    self.representation = json.load(open(f_source(\"DATA/P3/transformer.json\"),\"r\"))\n",
    "    self.representation = {key : np.array(self.representation[key]) for key in self.representation }\n",
    "  def query(self,input_str:str , k = 10):\n",
    "    q = self.model.encode(input_str)\n",
    "    article_id = sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])[:k]\n",
    "    article =  [self.documents[id[0]] for id in article_id]\n",
    "    return (article_id,article)\n",
    "        \n",
    "\n",
    "class Page_Ranking_Hits:\n",
    "    def __init__(self):\n",
    "        objective = \"article\"\n",
    "        self.ref_matrix = None\n",
    "        self.articles = sparse.load_npz(\"./DATA/P5/articles_sparse.npz\")\n",
    "        self.objective = self.articles if objective == \"article\" else self.authors\n",
    "        \n",
    "    def compute_page_rank(self, alpha = 0.9):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.pr = nx.pagerank(G=graph, alpha=alpha)\n",
    "        \n",
    "    def compute_hits(self):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.hub, self.authority = nx.hits(G=graph) \n",
    "        \n",
    "    def tops_pages(self, k = 10):\n",
    "        return sorted(self.pr.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "    \n",
    "    def cal_cites(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 0)).reshape(-1)\n",
    "    \n",
    "    def top_hubs(self, k = 10):\n",
    "        return sorted(self.hub.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "    \n",
    "    def top_auth(self, k = 10):\n",
    "        return sorted(self.authority.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "    \n",
    "    def cal_ref(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 1)).reshape(-1)\n",
    "\n",
    "MAIN_DATA_PATH = \"../DATA/clean_data.json\"\n",
    "CLUSTER_DATA_PATH = \"../DATA/clustring_data.csv\"\n",
    "\n",
    "\n",
    "class IR:\n",
    "    def __init__(self):\n",
    "        print(\"loading requirments ... \")\n",
    "        print(\"loading main data ... \")\n",
    "\n",
    "        self.main_data = json.load(open(address_resolver(MAIN_DATA_PATH),\"r\"))\n",
    "\n",
    "        print(\"loading clustring data ... \")\n",
    "        self.clustring_data = pd.read_csv(address_resolver(CLUSTER_DATA_PATH))\n",
    "        self.cluster_labels_map = {0:\"cs.LG\" , 1:\"cs.CV\" , 2:\"AI\" , 3:\"cs.RO\" , 4:\"cs.CL\"}\n",
    "        self.kmeas_map_label = {0: 2, 1: 2, 2: 1, 3: 3, 4: 2, 5: 1, 6: 1, 7: 4, 8: 1, 9: 1, 10: 1, 11: 1}\n",
    "        self.cluster_model = pickle.load(open(\"DATA/P4/finalized_cluster_model.sav\", 'rb'))\n",
    "        print(\"cluster_model = \",self.cluster_model)\n",
    "\n",
    "        print(\"loading Boolean search model\")\n",
    "        self.boolean_ir = Boolean_IR(self.main_data)\n",
    "        self.boolean_ir.pre_process_authors()\n",
    "        self.boolean_ir.pre_process_title()\n",
    "\n",
    "        print(\"loading tf-idf search model\")\n",
    "        self.tf_idf_raw = TF_IDF_IR(self.main_data)\n",
    "\n",
    "        print(\"loading fasttext module\")\n",
    "        self.fast_text = Fast_text_TF_IDF_IR(self.main_data,t = \"lemma\")\n",
    "        print(\"process fasttext module\")\n",
    "        self.fast_text.preprocess(pre = True ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )\n",
    "\n",
    "        print(\"Transformers loading\")\n",
    "        self.transformer = Transformer(self.main_data,\"all-MiniLM-L12-v2\")\n",
    "        self.bert_model = self.transformer.model\n",
    "\n",
    "        print(\"page_ranking_algorithm loading\")\n",
    "        page_hits_articles = Page_Ranking_Hits()\n",
    "        page_hits_articles.compute_page_rank(0.9)\n",
    "        page_hits_articles.compute_hits()\n",
    "\n",
    "\n",
    "        print(\"Finished loading packages.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def classification(self,text):\n",
    "        #text class\n",
    "        pass\n",
    "    \n",
    "    def clustring(self,text):\n",
    "        # abstract_bert = bert_model.encode(data_abstract,device = \"cuda\")\n",
    "        # titles_bert = bert_model.encode(data_title,device = \"cuda\")\n",
    "        #concated_data_bert = np.array([np.array([abstract_bert[i],titles_bert[i]]).reshape(-1) for i in range((abstract_bert.shape[0]))])\n",
    "        # text\n",
    "        # class\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def search(self,text,type_text,query_expansion = False,mode = \"bert\" , range_q = (0,10)):# abstract title author\n",
    "        #mode = bert , tf-idf , fasttext , boolean\n",
    "        #text_type = abstract , title , author\n",
    "        # range_q = (start , end)\n",
    "        pass\n",
    "    \n",
    "    def best_articles(self,type_query = \"page_rank\",mode = \"article\",k = 10):# page_rank,hits\n",
    "        # type_query = page_rank , hits ,\n",
    "        # k = numbers\n",
    "        # mode = artcile , author\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "ir = IR()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
