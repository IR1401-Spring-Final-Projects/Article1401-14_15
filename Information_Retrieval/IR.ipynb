{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading requirments ... \n",
      "loading main data ... \n",
      "loading fasttext module\n",
      "loading fasttext requirments\n",
      "process fasttext module\n",
      "training fasttext module\n",
      "\n",
      " making fastext\n",
      "make: Nothing to be done for 'fasttext/fastText'.\n",
      "./fasttext/fastText/fasttext module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './fasttext/word_embedding.*': No such file or directory\n",
      "sh: 1: ./fasttext/fastText/fasttext: not found\n",
      "rm: cannot remove './fasttext/word_embedding.bin': No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './fasttext/word_embedding.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 297>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=287'>288</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbest_articles\u001b[39m(\u001b[39mself\u001b[39m,type_query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpage_rank\u001b[39m\u001b[39m\"\u001b[39m,mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m,k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m):\u001b[39m# page_rank,hits\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=288'>289</a>\u001b[0m         \u001b[39m# type_query = page_rank , hits ,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=289'>290</a>\u001b[0m         \u001b[39m# k = numbers\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=290'>291</a>\u001b[0m         \u001b[39m# mode = artcile , author\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=292'>293</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=296'>297</a>\u001b[0m ir \u001b[39m=\u001b[39m IR()\n",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36mIR.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfast_text \u001b[39m=\u001b[39m Fast_text_TF_IDF_IR(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_data,t \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlemma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=265'>266</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mprocess fasttext module\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=266'>267</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfast_text\u001b[39m.\u001b[39;49mpreprocess(pre \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m ,dim\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m, epoch\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m , lr \u001b[39m=\u001b[39;49m \u001b[39m0.06\u001b[39;49m , ws \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m )\n",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 1\u001b[0m in \u001b[0;36mFast_text_TF_IDF_IR.preprocess\u001b[0;34m(self, pre, ws, epoch, lr, dim)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=212'>213</a>\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./fasttext/fastText/fasttext skipgram -dim \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m -ws \u001b[39m\u001b[39m{\u001b[39;00mws\u001b[39m}\u001b[39;00m\u001b[39m -epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m -lr \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m -input \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_path\u001b[39m}\u001b[39;00m\u001b[39m -output ./fasttext/word_embedding\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=213'>214</a>\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39m\"\u001b[39m\u001b[39mrm ./fasttext/word_embedding.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=214'>215</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memmbeding \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m\"\u001b[39;49m\u001b[39m./fasttext/word_embedding.vec\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=215'>216</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocuments:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000000?line=216'>217</a>\u001b[0m     article \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocuments[key]\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1723\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1678\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1679\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1680\u001b[0m     ):\n\u001b[1;32m   1681\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \n\u001b[1;32m   1683\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \n\u001b[1;32m   1722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1724\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1725\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1726\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2052\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2049\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2051\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2052\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2053\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2054\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './fasttext/word_embedding.vec'"
     ]
    }
   ],
   "source": [
    "# %%writefile mir.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enum\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download()\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "\n",
    "source_path = \"./\"\n",
    "def address_resolver(add):\n",
    "    return source_path + add\n",
    "\n",
    "def flatten(l : List[List]) -> List:\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "    ABSTRACT = \"Abstract Based\"\n",
    "class Boolean_IR:\n",
    "    def __init__(self,docs):\n",
    "        print(\"boolean search loading modules\")\n",
    "        self.author_to_id = json.load(open(\"DATA/P3/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/P3/author_to_doc.json\",\"r\"))\n",
    "        self.documents = docs\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/P3/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def word_tokenize_author(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        print(\"boolean search loading authors preprocess\")\n",
    "        self.all_names = list(set(flatten([self.word_tokenize_author(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_author(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        print(\"boolean search loading title preprocess\")\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "            \n",
    "    def title_ir(self,wk:str , k):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)\n",
    "            articles = [self.documents[id[0]] for id in mapping]\n",
    "            return (articles,mapping)\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_authoe(input_string.strip()),k) \n",
    "            articles = flatten([[self.documents[id] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])\n",
    "            return (articles[k[0]:k[1]],names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TF_IDF_IR:\n",
    "    def __init__(self,docs):\n",
    "\n",
    "        self.documents = docs\n",
    "        print(\"loading tf-idf model modules\")\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.lemma_abs = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "        self.idf_abs = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        self.idf_title = json.load(open(\"DATA/P3/idf_title.json\",\"r\"))\n",
    "        self.tf_title = json.load(open(\"DATA/P3/title_tf.json\",\"r\"))\n",
    "        self.tf_abs = json.load(open(\"DATA/P3/asb_tf.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "        for key in self.tf_title:\n",
    "            self.tf_title[key] = {int(k) : float(self.tf_title[key][k]) for k in self.tf_title[key]}\n",
    "        for key in self.tf_abs:\n",
    "            self.tf_abs[key] = {int(k) : float(self.tf_abs[key][k]) for k in self.tf_abs[key]}\n",
    "        self.lemma_title = {key : int(self.lemma_title[key]) for key in self.lemma_title}\n",
    "        self.lemma_abs = {key : int(self.lemma_abs[key]) for key in self.lemma_abs}\n",
    "        self.idf_abs =  {int(key) : float(self.idf_abs[key]) for key in self.idf_abs}\n",
    "        self.idf_title =  {int(key) : float(self.idf_title[key]) for key in self.idf_title}\n",
    "\n",
    "    def process_q(self,q : List , tf , idf , k) -> List[Tuple]:\n",
    "        return sorted([(key,sum([tf[key].get(wq,0) * idf.get(wq,0) for wq in q])) for key in tf], key = lambda x : x[1] , reverse=True)[:k]\n",
    "        \n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k : int = 10) -> List:\n",
    "        wk = self.tokenizer(input_string.strip().lower())\n",
    "        if type == Query_type.TITLE:\n",
    "            q = [int(self.lemma_title.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_title,self.idf_title , k)\n",
    "        elif type == Query_type.ABSTRACT:\n",
    "            q = [int(self.lemma_abs.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_abs,self.idf_abs , k)\n",
    "        articles = [self.documents[id[0]] for id in result]\n",
    "        return (articles,result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,docs,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = docs\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        print(\"loading fasttext requirments\")\n",
    "        if t == \"lemma\":  \n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            print(\"training fasttext module\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            print(\"\\n making fastext\")\n",
    "            os.system(\"make ./fasttext/fastText\")\n",
    "            print(\"./fasttext/fastText/fasttext module\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array) -> List[Tuple]:\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        \n",
    "\n",
    "    def query(self, input_string:str , k : int = 10) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q)[:k]\n",
    "        articles = [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,articles)\n",
    "\n",
    "        \n",
    "MAIN_DATA_PATH = \"../DATA/clean_data.json\"\n",
    "CLUSTER_DATA_PATH = \"../DATA/clustring_data.csv\"\n",
    "\n",
    "class IR:\n",
    "    def __init__(self):\n",
    "        print(\"loading requirments ... \")\n",
    "        print(\"loading main data ... \")\n",
    "        self.main_data = json.load(open(address_resolver(MAIN_DATA_PATH),\"r\"))\n",
    "        # print(\"loading clustring data ... \")\n",
    "        # self.clustring_data = pd.read_csv(address_resolver(CLUSTER_DATA_PATH))\n",
    "        # print(\"loading Boolean search model\")\n",
    "        # self.boolean_ir = Boolean_IR(self.main_data)\n",
    "        # self.boolean_ir.pre_process_authors()\n",
    "        # self.boolean_ir.pre_process_title()\n",
    "        # print(\"loading tf-idf search model\")\n",
    "        # self.tf_idf_raw = TF_IDF_IR(self.main_data)\n",
    "        print(\"loading fasttext module\")\n",
    "        self.fast_text = Fast_text_TF_IDF_IR(self.main_data,t = \"lemma\")\n",
    "        print(\"process fasttext module\")\n",
    "        self.fast_text.preprocess(pre = False ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def classification(self,text):\n",
    "        #text class\n",
    "        pass\n",
    "    \n",
    "    def clustring(self,text):#GMM\n",
    "        # text\n",
    "        # class\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def search(self,text,type_text,query_expansion = False,mode = \"bert\" , range_q = (0,10)):# abstract title author\n",
    "        #mode = bert , tf-idf , fasttext , boolean\n",
    "        #text_type = abstract , title , author\n",
    "        # range_q = (start , end)\n",
    "        pass\n",
    "    \n",
    "    def best_articles(self,type_query = \"page_rank\",mode = \"article\",k = 10):# page_rank,hits\n",
    "        # type_query = page_rank , hits ,\n",
    "        # k = numbers\n",
    "        # mode = artcile , author\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "ir = IR()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir\n",
    "ir = mir.IR(\"./\")\n",
    "#ir.boolean_ir.author_ir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
