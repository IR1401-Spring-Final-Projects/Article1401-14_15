{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading requirments ... \n",
      "loading main data ... \n",
      "Transformers loading\n",
      "Transformer\n",
      "downloading model all-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# %%writefile mir.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enum\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download()\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "# pip install -U sentence-transformers\n",
    "source_path = \"./\"\n",
    "def address_resolver(add):\n",
    "    return source_path + add\n",
    "\n",
    "def flatten(l : List[List]) -> List:\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "    ABSTRACT = \"Abstract Based\"\n",
    "class Boolean_IR:\n",
    "    def __init__(self,docs):\n",
    "        print(\"boolean search loading modules\")\n",
    "        self.author_to_id = json.load(open(\"DATA/P3/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/P3/author_to_doc.json\",\"r\"))\n",
    "        self.documents = docs\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/P3/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def word_tokenize_author(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        print(\"boolean search loading authors preprocess\")\n",
    "        self.all_names = list(set(flatten([self.word_tokenize_author(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_author(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        print(\"boolean search loading title preprocess\")\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "            \n",
    "    def title_ir(self,wk:str , k):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)\n",
    "            articles = [self.documents[id[0]] for id in mapping]\n",
    "            return (articles,mapping)\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_authoe(input_string.strip()),k) \n",
    "            articles = flatten([[self.documents[id] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])\n",
    "            return (articles[k[0]:k[1]],names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TF_IDF_IR:\n",
    "    def __init__(self,docs):\n",
    "\n",
    "        self.documents = docs\n",
    "        print(\"loading tf-idf model modules\")\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.lemma_abs = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "        self.idf_abs = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        self.idf_title = json.load(open(\"DATA/P3/idf_title.json\",\"r\"))\n",
    "        self.tf_title = json.load(open(\"DATA/P3/title_tf.json\",\"r\"))\n",
    "        self.tf_abs = json.load(open(\"DATA/P3/asb_tf.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "        for key in self.tf_title:\n",
    "            self.tf_title[key] = {int(k) : float(self.tf_title[key][k]) for k in self.tf_title[key]}\n",
    "        for key in self.tf_abs:\n",
    "            self.tf_abs[key] = {int(k) : float(self.tf_abs[key][k]) for k in self.tf_abs[key]}\n",
    "        self.lemma_title = {key : int(self.lemma_title[key]) for key in self.lemma_title}\n",
    "        self.lemma_abs = {key : int(self.lemma_abs[key]) for key in self.lemma_abs}\n",
    "        self.idf_abs =  {int(key) : float(self.idf_abs[key]) for key in self.idf_abs}\n",
    "        self.idf_title =  {int(key) : float(self.idf_title[key]) for key in self.idf_title}\n",
    "\n",
    "    def process_q(self,q : List , tf , idf , k) -> List[Tuple]:\n",
    "        return sorted([(key,sum([tf[key].get(wq,0) * idf.get(wq,0) for wq in q])) for key in tf], key = lambda x : x[1] , reverse=True)[:k]\n",
    "        \n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k : int = 10) -> List:\n",
    "        wk = self.tokenizer(input_string.strip().lower())\n",
    "        if type == Query_type.TITLE:\n",
    "            q = [int(self.lemma_title.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_title,self.idf_title , k)\n",
    "        elif type == Query_type.ABSTRACT:\n",
    "            q = [int(self.lemma_abs.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_abs,self.idf_abs , k)\n",
    "        articles = [self.documents[id[0]] for id in result]\n",
    "        return (articles,result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,docs,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = docs\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        print(\"loading fasttext requirments\")\n",
    "        if t == \"lemma\":  \n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            print(\"training fasttext module\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            print(\"\\n making fastext\")\n",
    "            os.chdir('./fasttext/fastText')\n",
    "            os.system(\"make\")\n",
    "            os.chdir('../..')\n",
    "            print(os.getcwd())\n",
    "            print(\"./fasttext/fastText/fasttext module\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array) -> List[Tuple]:\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        \n",
    "\n",
    "    def query(self, input_string:str , k : int = 10) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q)[:k]\n",
    "        articles = [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,articles)\n",
    "\n",
    "source = \"./\"\n",
    "f_source = lambda s : source+\"/\"+s\n",
    "class Transformer:\n",
    "  def __init__(self,docs,model_name = 'all-MiniLM-L6-v2'):\n",
    "    print(f\"Transformer\\ndownloading model {model_name}\")\n",
    "    self.model = SentenceTransformer(model_name)\n",
    "    self.documents = docs\n",
    "    self.representation = None\n",
    "  \n",
    "  def preprocess(self,pre_use = False):\n",
    "    if not pre_use:\n",
    "      docs = []\n",
    "      keys = []\n",
    "      print(f\"creating representation for docs\")\n",
    "      for key in self.documents:\n",
    "        abstract = self.documents[key][\"abstract\"]\n",
    "        if type(abstract) == str:\n",
    "          docs.append(abstract)\n",
    "          keys.append(key)\n",
    "      embeddings = self.model.encode(docs)\n",
    "      self.representation = {}\n",
    "      for key, embedding in zip(keys, embeddings):\n",
    "        self.representation[key] = embedding.tolist()\n",
    "      addr = f_source(\"DATA/P3/transformer.json\")\n",
    "      print(f\"saving docs_rep in {addr}\")\n",
    "      open(addr,\"w\").write(json.dumps(self.representation))\n",
    "    print(f\"loading docs_rep\")\n",
    "    self.representation = json.load(open(f_source(\"DATA/P3/transformer.json\"),\"r\"))\n",
    "    self.representation = {key : np.array(self.representation[key]) for key in self.representation }\n",
    "  def query(self,input_str:str , k = 10):\n",
    "    q = self.model.encode(input_str)\n",
    "    article_id = sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])[:k]\n",
    "    article =  [self.documents[id[0]] for id in article_id]\n",
    "    return (article_id,article)\n",
    "        \n",
    "MAIN_DATA_PATH = \"../DATA/clean_data.json\"\n",
    "CLUSTER_DATA_PATH = \"../DATA/clustring_data.csv\"\n",
    "\n",
    "class IR:\n",
    "    def __init__(self):\n",
    "        print(\"loading requirments ... \")\n",
    "        print(\"loading main data ... \")\n",
    "        self.main_data = json.load(open(address_resolver(MAIN_DATA_PATH),\"r\"))\n",
    "        # print(\"loading clustring data ... \")\n",
    "        # self.clustring_data = pd.read_csv(address_resolver(CLUSTER_DATA_PATH))\n",
    "        # print(\"loading Boolean search model\")\n",
    "        # self.boolean_ir = Boolean_IR(self.main_data)\n",
    "        # self.boolean_ir.pre_process_authors()\n",
    "        # self.boolean_ir.pre_process_title()\n",
    "        # print(\"loading tf-idf search model\")\n",
    "        # self.tf_idf_raw = TF_IDF_IR(self.main_data)\n",
    "        # print(\"loading fasttext module\")\n",
    "        # self.fast_text = Fast_text_TF_IDF_IR(self.main_data,t = \"lemma\")\n",
    "        # print(\"process fasttext module\")\n",
    "        # self.fast_text.preprocess(pre = True ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )\n",
    "        print(\"Transformers loading\")\n",
    "        self.transformer = Transformer(self.main_data,\"all-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def classification(self,text):\n",
    "        #text class\n",
    "        pass\n",
    "    \n",
    "    def clustring(self,text):#GMM\n",
    "        # text\n",
    "        # class\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def search(self,text,type_text,query_expansion = False,mode = \"bert\" , range_q = (0,10)):# abstract title author\n",
    "        #mode = bert , tf-idf , fasttext , boolean\n",
    "        #text_type = abstract , title , author\n",
    "        # range_q = (start , end)\n",
    "        pass\n",
    "    \n",
    "    def best_articles(self,type_query = \"page_rank\",mode = \"article\",k = 10):# page_rank,hits\n",
    "        # type_query = page_rank , hits ,\n",
    "        # k = numbers\n",
    "        # mode = artcile , author\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "ir = IR()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
