{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%writefile mir.py\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mir.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mir.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import enum\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline\n",
    "\n",
    "# nltk.download()\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "# pip install -U sentence-transformers\n",
    "\n",
    "source_path = \"./\"\n",
    "def address_resolver(add):\n",
    "    return source_path + add\n",
    "\n",
    "def flatten(l : List[List]) -> List:\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "    ABSTRACT = \"Abstract Based\"\n",
    "class Boolean_IR:\n",
    "    def __init__(self,docs):\n",
    "        print(\"boolean search loading modules\")\n",
    "        self.author_to_id = json.load(open(\"DATA/P3/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/P3/author_to_doc.json\",\"r\"))\n",
    "        self.documents = docs\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/P3/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def word_tokenize_author(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        print(\"boolean search loading authors preprocess\")\n",
    "        self.all_names = list(set(flatten([self.word_tokenize_author(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_author(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        print(\"boolean search loading title preprocess\")\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "\n",
    "    def title_ir(self,wk:str , k):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)[k[0]:k[1]]\n",
    "            return mapping\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_author(input_string.strip()),k)\n",
    "            articles = flatten([[self.documents[id][\"paperId\"] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])[k[0]:k[1]]\n",
    "            return (articles,names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TF_IDF_IR:\n",
    "    def __init__(self,docs):\n",
    "\n",
    "        self.documents = docs\n",
    "        print(\"loading tf-idf model modules\")\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.lemma_abs = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "        self.idf_abs = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        self.idf_title = json.load(open(\"DATA/P3/idf_title.json\",\"r\"))\n",
    "        self.tf_title = json.load(open(\"DATA/P3/title_tf.json\",\"r\"))\n",
    "        self.tf_abs = json.load(open(\"DATA/P3/asb_tf.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "        for key in self.tf_title:\n",
    "            self.tf_title[key] = {int(k) : float(self.tf_title[key][k]) for k in self.tf_title[key]}\n",
    "        for key in self.tf_abs:\n",
    "            self.tf_abs[key] = {int(k) : float(self.tf_abs[key][k]) for k in self.tf_abs[key]}\n",
    "        self.lemma_title = {key : int(self.lemma_title[key]) for key in self.lemma_title}\n",
    "        self.lemma_abs = {key : int(self.lemma_abs[key]) for key in self.lemma_abs}\n",
    "        self.idf_abs =  {int(key) : float(self.idf_abs[key]) for key in self.idf_abs}\n",
    "        self.idf_title =  {int(key) : float(self.idf_title[key]) for key in self.idf_title}\n",
    "\n",
    "    def process_q(self,q : List , tf , idf , k) -> List[Tuple]:\n",
    "        without_expansion = sorted([(key,sum([tf[key].get(wq,0) * idf.get(wq,0) for wq in q])) for key in tf], key = lambda x : x[1] , reverse=True)[k[0]:k[1]]\n",
    "        return without_expansion\n",
    "\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> List:\n",
    "        wk = self.tokenizer(input_string.strip().lower())\n",
    "        if type == Query_type.TITLE:\n",
    "            q = [int(self.lemma_title.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_title,self.idf_title , k)\n",
    "        elif type == Query_type.ABSTRACT:\n",
    "            q = [int(self.lemma_abs.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_abs,self.idf_abs , k)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,docs,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = docs\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        print(\"loading fasttext requirments\")\n",
    "        if t == \"lemma\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            print(\"training fasttext module\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            print(\"\\n making fastext\")\n",
    "            os.chdir('./fasttext/fastText')\n",
    "            os.system(\"make\")\n",
    "            os.chdir('../..')\n",
    "            print(os.getcwd())\n",
    "            print(\"./fasttext/fastText/fasttext module\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array , expansion = False) -> List[Tuple]:\n",
    "        without_expansion = sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        if not expansion :\n",
    "            return without_expansion\n",
    "        near = without_expansion[:10]\n",
    "        far = without_expansion[-10:]\n",
    "        q = 0.6 * q + 0.5 * np.mean([self.doc_emb[a[0]]for a in near],axis = 0) - 0.1 * np.mean([self.doc_emb[a[0]]for a in far], axis = 0)\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "\n",
    "    def query(self, input_string:str , k, expansion = False) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q , expansion)[k[0]:k[1]]\n",
    "        return article_id\n",
    "\n",
    "source = \"./\"\n",
    "f_source = lambda s : source+\"/\"+s\n",
    "class Transformer:\n",
    "  def __init__(self,docs,model_name = './DATA/sentence-transformers_all-MiniLM-L12-v2/'):\n",
    "    print(f\"Transformer\\ndownloading model {model_name}\")\n",
    "    self.model = SentenceTransformer(model_name)\n",
    "    self.documents = docs\n",
    "    self.representation = None\n",
    "\n",
    "  def preprocess(self,pre_use = False):\n",
    "    if not pre_use:\n",
    "      docs = []\n",
    "      keys = []\n",
    "      print(f\"creating representation for docs\")\n",
    "      for key in self.documents:\n",
    "        abstract = self.documents[key][\"abstract\"]\n",
    "        if type(abstract) == str:\n",
    "          docs.append(abstract)\n",
    "          keys.append(key)\n",
    "      embeddings = self.model.encode(docs)\n",
    "      self.representation = {}\n",
    "      for key, embedding in zip(keys, embeddings):\n",
    "        self.representation[key] = embedding.tolist()\n",
    "      addr = f_source(\"DATA/P3/transformer.json\")\n",
    "      print(f\"saving docs_rep in {addr}\")\n",
    "      open(addr,\"w\").write(json.dumps(self.representation))\n",
    "    print(f\"loading docs_rep\")\n",
    "    self.representation = json.load(open(f_source(\"DATA/P3/transformer.json\"),\"r\"))\n",
    "    self.representation = {key : np.array(self.representation[key]) for key in self.representation }\n",
    "  def query(self,input_str:str , k , expansion = False):\n",
    "    q = self.model.encode(input_str)\n",
    "    without_expansion = sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])\n",
    "    if not expansion:\n",
    "        article_id = without_expansion[k[0]:k[1]]\n",
    "        article =  [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,article)\n",
    "    near = without_expansion[:10]\n",
    "    far = without_expansion[-10:]\n",
    "    q = 0.6 * q + 0.5 * np.mean([self.representation[a[0]]for a in near],axis = 0) - 0.1 * np.mean([self.representation[a[0]]for a in far],axis = 0)\n",
    "    return sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])[k[0]:k[1]]\n",
    "\n",
    "\n",
    "class Page_Ranking_Hits:\n",
    "    def __init__(self):\n",
    "        objective = \"article\"\n",
    "        self.ref_matrix = None\n",
    "        self.articles = sparse.load_npz(\"./DATA/P5/articles_sparse.npz\")\n",
    "        self.objective = self.articles if objective == \"article\" else self.authors\n",
    "        representation = json.load(open(f_source(\"DATA/P5/article_mapping.json\"),\"r\"))\n",
    "        self.mapping = {int(representation[doc]):doc for doc in representation}\n",
    "\n",
    "    def compute_page_rank(self, alpha = 0.9):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.pr = nx.pagerank(G=graph, alpha=alpha)\n",
    "\n",
    "    def compute_hits(self):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.hub, self.authority = nx.hits(G=graph)\n",
    "\n",
    "    def tops_pages(self, k = 10):\n",
    "        res = sorted(self.pr.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "        return [self.mapping[int(ar[0])] for ar in res]\n",
    "\n",
    "    def cal_cites(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 0)).reshape(-1)\n",
    "\n",
    "    def top_hubs(self, k = 10):\n",
    "        return sorted(self.hub.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "\n",
    "    def top_auth(self, k = 10):\n",
    "        return sorted(self.authority.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "\n",
    "    def cal_ref(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 1)).reshape(-1)\n",
    "\n",
    "MAIN_DATA_PATH = \"../DATA/clean_data.json\"\n",
    "CLUSTER_DATA_PATH = \"../DATA/clustring_data.csv\"\n",
    "CLASSIFICATION_DATA_PATH = \"../DATA/classification_data.csv\"\n",
    "\n",
    "\n",
    "class IR:\n",
    "    def __init__(self):\n",
    "        print(\"loading requirments ... \")\n",
    "        print(\"loading main data ... \")\n",
    "\n",
    "        self.main_data = json.load(open(address_resolver(MAIN_DATA_PATH),\"r\"))\n",
    "\n",
    "        print(\"loading clustring data ... \")\n",
    "        self.clustring_data = pd.read_csv(address_resolver(CLUSTER_DATA_PATH))\n",
    "        self.cluster_labels_map = {0:\"cs.LG\" , 1:\"cs.CV\" , 2:\"cs.AI\" , 3:\"cs.RO\" , 4:\"cs.CL\"}\n",
    "        self.kmeas_map_label = {0: 2, 1: 2, 2: 1, 3: 3, 4: 2, 5: 1, 6: 1, 7: 4, 8: 1, 9: 1, 10: 1, 11: 1}\n",
    "        self.cluster_model = pickle.load(open(\"DATA/P4/finalized_cluster_model.sav\", 'rb'))\n",
    "        print(\"cluster_model = \",self.cluster_model)\n",
    "\n",
    "        print(\"loading Boolean search model\")\n",
    "        self.boolean_ir = Boolean_IR(self.main_data)\n",
    "        self.boolean_ir.pre_process_authors()\n",
    "        self.boolean_ir.pre_process_title()\n",
    "\n",
    "        print(\"loading tf-idf search model\")\n",
    "        self.tf_idf_raw = TF_IDF_IR(self.main_data)\n",
    "\n",
    "        print(\"loading fasttext module\")\n",
    "        self.fast_text = Fast_text_TF_IDF_IR(self.main_data,t = \"lemma\")\n",
    "        print(\"process fasttext module\")\n",
    "        self.fast_text.preprocess(pre = True ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )\n",
    "\n",
    "        print(\"Transformers loading\")\n",
    "        self.transformer = Transformer(self.main_data,'./DATA/sentence-transformers_all-MiniLM-L12-v2/')\n",
    "        self.transformer.preprocess(pre_use = True)\n",
    "        self.bert_model = self.transformer.model\n",
    "\n",
    "        print(\"page_ranking_algorithm loading\")\n",
    "        self.page_hits_articles = Page_Ranking_Hits()\n",
    "        self.page_hits_articles.compute_page_rank(0.9)\n",
    "        self.page_hits_articles.compute_hits()\n",
    "\n",
    "        print(\"loading classification data ... \")\n",
    "        self.classification_model_name  = 'distilbert-base-uncased'\n",
    "        self.classification_model = AutoModelForSequenceClassification.from_pretrained(\"DATA/P4/classification_model\", from_tf=True)\n",
    "        self.classification_classes = {\n",
    "                        'LABEL_0' : 'cs.CV',\n",
    "                        'LABEL_1' : 'cs.LG',\n",
    "                        'LABEL_2' : 'stat.ML'\n",
    "                    }\n",
    "        self.classification_class_categories = {v: k for k, v in self.classification_classes.items()}\n",
    "        self.classification_tokenizer = AutoTokenizer.from_pretrained(self.classification_model_name)\n",
    "        self.classification_pipeline = TextClassificationPipeline(model=self.classification_model, tokenizer=self.classification_tokenizer, return_all_scores=False)\n",
    "\n",
    "        print(\"Finished loading packages.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def classification(self,text):\n",
    "        prediction = self.classification_pipeline(text)[0]\n",
    "        predicted_class = self.classification_classes[prediction['label']]\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "\n",
    "    def clustring(self,text):\n",
    "        # abstract_bert = bert_model.encode(data_abstract,device = \"cuda\")\n",
    "        # titles_bert = bert_model.encode(data_title,device = \"cuda\")\n",
    "        #concated_data_bert = np.array([np.array([abstract_bert[i],titles_bert[i]]).reshape(-1) for i in range((abstract_bert.shape[0]))])\n",
    "        # text\n",
    "        # class\n",
    "        abstract_bert = self.bert_model.encode([text])\n",
    "        titles_bert = self.bert_model.encode([\"title\"])\n",
    "        concated_data_bert = np.array([np.array([abstract_bert[i],titles_bert[i]]).reshape(-1) for i in range((abstract_bert.shape[0]))])\n",
    "        return self.cluster_labels_map[self.kmeas_map_label[self.cluster_model.predict(concated_data_bert)[0]]]\n",
    "\n",
    "\n",
    "    def search(self,text,type_text,query_expansion = False,mode = \"bert\" , range_q = (0,40)):# abstract title author\n",
    "        #mode = bert , tf-idf , fasttext , boolean\n",
    "        #text_type = abstract , title , author\n",
    "        # range_q = (start , end)\n",
    "        if mode == \"bert\":\n",
    "            return self.transformer.query(text,k = range_q , expansion=query_expansion)\n",
    "        elif mode == \"fasttext\":\n",
    "            return self.fast_text.query(text,range_q,query_expansion)\n",
    "        elif mode == \"tf_idf\":\n",
    "            qt = Query_type.ABSTRACT\n",
    "            if type_text == \"title\":\n",
    "                qt = Query_type.TITLE\n",
    "            return self.tf_idf_raw.query(qt, text , range_q)\n",
    "        elif mode == \"boolean\":\n",
    "            qt = Query_type.TITLE\n",
    "            if type_text == \"author\":\n",
    "                qt = Query_type.AUTHOR\n",
    "            return self.boolean_ir.query(qt,text,range_q)\n",
    "\n",
    "    def best_articles(self,k = 10):# page_rank,hits\n",
    "        return self.page_hits_articles.tops_pages(k = 10)\n",
    "        # type_query = page_rank , hits ,\n",
    "        # k = numbers\n",
    "        # mode = artcile , author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
