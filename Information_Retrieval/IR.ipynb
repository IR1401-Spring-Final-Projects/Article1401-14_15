{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%writefile mir.py\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading requirments ... \n",
      "loading main data ... \n",
      "loading clustring data ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirhossein/anaconda3/envs/AI/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator KMeans from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_model =  KMeans(algorithm='auto', max_iter=1000, n_clusters=12)\n",
      "loading Boolean search model\n",
      "boolean search loading modules\n",
      "boolean search loading authors preprocess\n",
      "boolean search loading title preprocess\n",
      "loading tf-idf search model\n",
      "loading tf-idf model modules\n",
      "loading fasttext module\n",
      "loading fasttext requirments\n",
      "process fasttext module\n",
      "Transformers loading\n",
      "Transformer\n",
      "downloading model ./DATA/sentence-transformers_all-MiniLM-L12-v2/\n",
      "loading docs_rep\n",
      "page_ranking_algorithm loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirhossein/anaconda3/envs/AI/lib/python3.10/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading classification data ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\n",
      "Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 449>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=441'>442</a>\u001b[0m         \u001b[39m#self.page_hits_articles.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=442'>443</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=443'>444</a>\u001b[0m         \u001b[39m# type_query = page_rank , hits ,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=444'>445</a>\u001b[0m         \u001b[39m# k = numbers\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=445'>446</a>\u001b[0m         \u001b[39m# mode = artcile , author\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=447'>448</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=448'>449</a>\u001b[0m ir \u001b[39m=\u001b[39m IR()\n",
      "\u001b[1;32m/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb Cell 2\u001b[0m in \u001b[0;36mIR.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=377'>378</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloading classification data ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=378'>379</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_model_name  \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=379'>380</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mDATA/P4/classification_model\u001b[39;49m\u001b[39m\"\u001b[39;49m, from_tf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=380'>381</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_classes \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=381'>382</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mLABEL_0\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mcs.CV\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=382'>383</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mLABEL_1\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mcs.LG\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=383'>384</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mLABEL_2\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m'\u001b[39m\u001b[39mstat.ML\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=384'>385</a>\u001b[0m             }\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amirhossein/Documents/Term6/IR/Article1401-14_15/Information_Retrieval/IR.ipynb#ch0000001?line=385'>386</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_class_categories \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_classes\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    445\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    447\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/transformers/modeling_utils.py:1859\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1857\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[0;32m-> 1859\u001b[0m     model \u001b[39m=\u001b[39m load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1860\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   1861\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[1;32m   1862\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1863\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.10/site-packages/transformers/modeling_tf_pytorch_utils.py:293\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[0;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mLoad TF 2.0 HDF5 checkpoint in a PyTorch model We use HDF5 to easily do transfer learning (see\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mhttps://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/python/keras/engine/network.py#L1352-L1357).\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import enum\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import List,Tuple\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline\n",
    "\n",
    "# nltk.download()\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "# pip install -U sentence-transformers\n",
    "\n",
    "source_path = \"./\"\n",
    "def address_resolver(add):\n",
    "    return source_path + add\n",
    "\n",
    "def flatten(l : List[List]) -> List:\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class Query_type(enum.Enum):\n",
    "    AUTHOR = \"Author Based\"\n",
    "    TITLE = \"Title Based\"\n",
    "    ABSTRACT = \"Abstract Based\"\n",
    "class Boolean_IR:\n",
    "    def __init__(self,docs):\n",
    "        print(\"boolean search loading modules\")\n",
    "        self.author_to_id = json.load(open(\"DATA/P3/author_to_id.json\",\"r\"))\n",
    "        self.author_to_doc = json.load(open(\"DATA/P3/author_to_doc.json\",\"r\"))\n",
    "        self.documents = docs\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.bool_dic_title = json.load(open(\"DATA/P3/bool_dic_title.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.title_tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "\n",
    "    def word_tokenize_author(self,t : str) -> List:\n",
    "        res = word_tokenize(t)\n",
    "        if (res[-1] != \".\"):\n",
    "            return res\n",
    "        res[-2] = res[-2]+res[-1]\n",
    "        return res[:-1]\n",
    "\n",
    "    def pre_process_authors(self) -> None:\n",
    "        print(\"boolean search loading authors preprocess\")\n",
    "        self.all_names = list(set(flatten([self.word_tokenize_author(key) for key in self.author_to_id if not is_int(key)])))\n",
    "        i = iter(range(1,len(self.all_names)+1))\n",
    "        self.w_mapping = defaultdict(lambda : next(i))\n",
    "        self.bool_dic_author = defaultdict(lambda : [])\n",
    "        list(map(lambda x : self.w_mapping[x],self.all_names))\n",
    "        removed_key = []\n",
    "        for key in self.author_to_id:\n",
    "            if not is_int(key) and is_int(self.author_to_id[key]) and key:\n",
    "                i = self.author_to_id[key]\n",
    "                self.bool_dic_author[i] = np.array([self.w_mapping[w] for w in self.word_tokenize_author(key)])\n",
    "            else:\n",
    "                removed_key.append(key)\n",
    "        for x in removed_key:\n",
    "            del self.author_to_id[x]\n",
    "    def pre_process_title(self) -> None:\n",
    "        print(\"boolean search loading title preprocess\")\n",
    "        for key in self.bool_dic_title:\n",
    "            self.bool_dic_title[key] = np.array(self.bool_dic_title[key])\n",
    "\n",
    "    def title_ir(self,wk:str , k):\n",
    "        words = np.array([self.lemma_title.get(w,0) for w in wk])\n",
    "        titles = [(key,np.sum([np.sum([item == self.bool_dic_title[key] for item in words ])])) for key in self.documents if type(self.documents[key][\"title\"]) == str]\n",
    "        return sorted(titles , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def author_ir(self,input_wk:str,k) -> List:\n",
    "        names_map = np.array([self.w_mapping.get(w,0) for w in input_wk])\n",
    "        authors = [(key,np.sum([np.sum([name == self.bool_dic_author[self.author_to_id[key]] for name in names_map ])])) for key in self.author_to_id]\n",
    "        return sorted(authors , key = lambda x : x[1],reverse=True)[k[0]:k[1]]\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> Tuple[List,List]:\n",
    "        input_string = input_string.lower()\n",
    "        if type == Query_type.TITLE:\n",
    "            mapping = self.title_ir(self.title_tokenizer(input_string.strip().lower()), k)\n",
    "            articles = [self.documents[id[0]] for id in mapping]\n",
    "            return (articles,mapping)\n",
    "        elif type == Query_type.AUTHOR:\n",
    "            names =  self.author_ir(self.word_tokenize_authoe(input_string.strip()),k)\n",
    "            articles = flatten([[self.documents[id] for id in self.author_to_doc[self.author_to_id[name[0]]]] for name in names])\n",
    "            return (articles[k[0]:k[1]],names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TF_IDF_IR:\n",
    "    def __init__(self,docs):\n",
    "\n",
    "        self.documents = docs\n",
    "        print(\"loading tf-idf model modules\")\n",
    "        self.lemma_title = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "        self.lemma_abs = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "        self.idf_abs = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        self.idf_title = json.load(open(\"DATA/P3/idf_title.json\",\"r\"))\n",
    "        self.tf_title = json.load(open(\"DATA/P3/title_tf.json\",\"r\"))\n",
    "        self.tf_abs = json.load(open(\"DATA/P3/asb_tf.json\",\"r\"))\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s) if token.lemma_ not in self.nlp.Defaults.stop_words ]\n",
    "        for key in self.tf_title:\n",
    "            self.tf_title[key] = {int(k) : float(self.tf_title[key][k]) for k in self.tf_title[key]}\n",
    "        for key in self.tf_abs:\n",
    "            self.tf_abs[key] = {int(k) : float(self.tf_abs[key][k]) for k in self.tf_abs[key]}\n",
    "        self.lemma_title = {key : int(self.lemma_title[key]) for key in self.lemma_title}\n",
    "        self.lemma_abs = {key : int(self.lemma_abs[key]) for key in self.lemma_abs}\n",
    "        self.idf_abs =  {int(key) : float(self.idf_abs[key]) for key in self.idf_abs}\n",
    "        self.idf_title =  {int(key) : float(self.idf_title[key]) for key in self.idf_title}\n",
    "\n",
    "    def process_q(self,q : List , tf , idf , k) -> List[Tuple]:\n",
    "        without_expansion = sorted([(key,sum([tf[key].get(wq,0) * idf.get(wq,0) for wq in q])) for key in tf], key = lambda x : x[1] , reverse=True)[k[0]:k[1]]\n",
    "        return without_expansion\n",
    "\n",
    "\n",
    "    def query(self,type : Query_type , input_string:str , k) -> List:\n",
    "        wk = self.tokenizer(input_string.strip().lower())\n",
    "        if type == Query_type.TITLE:\n",
    "            q = [int(self.lemma_title.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_title,self.idf_title , k)\n",
    "        elif type == Query_type.ABSTRACT:\n",
    "            q = [int(self.lemma_abs.get(w,0)) for w in wk]\n",
    "            result = self.process_q(q,self.tf_abs,self.idf_abs , k)\n",
    "        articles = [self.documents[id[0]] for id in result]\n",
    "        return (articles,result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    y = np.exp(x - np.max(x))\n",
    "    return y / y.sum()\n",
    "\n",
    "class Fast_text_TF_IDF_IR:\n",
    "    def __init__(self,docs,t = \"lemma\" , c_soft = True):\n",
    "        self.documents = docs\n",
    "        self.t = t\n",
    "        self.mapping = None\n",
    "        self.idf = None\n",
    "        self.train_data_path = None\n",
    "        print(\"loading fasttext requirments\")\n",
    "        if t == \"lemma\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.tokenizer = lambda s : [token.lemma_ for token in self.nlp(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract.json\",\"r\"))\n",
    "        else:\n",
    "            self.tokenizer = lambda s : [token for token in word_tokenize(s)]\n",
    "            self.train_data_path = \"./fasttext/fasttext_not_lemma_data.txt\"\n",
    "            self.mapping = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "            self.idf = json.load(open(\"DATA/P3/idf_abstract_not_lemma.json\",\"r\"))\n",
    "        self.emmbeding = None\n",
    "        self.mapping = {key : int(self.mapping[key]) for key in self.mapping}\n",
    "        self.idf = {int(key) : float(self.idf[key]) for key in self.idf}\n",
    "        self.c_soft = lambda x : x\n",
    "        if c_soft:\n",
    "            self.c_soft = lambda x : softmax(x)\n",
    "        self.doc_emb = {}\n",
    "        self.dim = 300\n",
    "    def is_c_(self,w):\n",
    "        try:\n",
    "            self.emmbeding[w]\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def preprocess(self,pre = False , ws = 5 ,epoch = 20 ,lr = 0.1, dim = 200):\n",
    "        self.dim = dim\n",
    "        if not pre:\n",
    "            print(\"training fasttext module\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.*\")\n",
    "            print(\"\\n making fastext\")\n",
    "            os.chdir('./fasttext/fastText')\n",
    "            os.system(\"make\")\n",
    "            os.chdir('../..')\n",
    "            print(os.getcwd())\n",
    "            print(\"./fasttext/fastText/fasttext module\")\n",
    "            os.system(f\"./fasttext/fastText/fasttext skipgram -dim {dim} -ws {ws} -epoch {epoch} -lr {lr} -input {self.train_data_path} -output ./fasttext/word_embedding\")\n",
    "            os.system(\"rm ./fasttext/word_embedding.bin\")\n",
    "            self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "            for key in self.documents:\n",
    "                article = self.documents[key]\n",
    "                abstract = article[\"abstract\"]\n",
    "                if (type(abstract) == str):\n",
    "                    try:\n",
    "                        word = self.tokenizer(abstract)\n",
    "                        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "                        c = np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1)\n",
    "                        c = self.c_soft(c)\n",
    "                        self.doc_emb[key] = np.matmul(c,matrix).tolist()[0]\n",
    "                    except:\n",
    "                        print(key,c)\n",
    "            open(\"./fasttext/doc_embedding.json\",\"w\").write(json.dumps(self.doc_emb))\n",
    "\n",
    "        self.emmbeding = KeyedVectors.load_word2vec_format(\"./fasttext/word_embedding.vec\")\n",
    "        self.doc_emb = json.load(open(\"./fasttext/doc_embedding.json\",\"r\"))\n",
    "        self.doc_emb = {key : np.array(self.doc_emb[key]).reshape(1,self.dim) for key in self.doc_emb}\n",
    "\n",
    "    def process_q(self,q : np.array , expansion = False) -> List[Tuple]:\n",
    "        without_expansion = sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "        if not expansion :\n",
    "            return without_expansion\n",
    "        near = without_expansion[:10]\n",
    "        far = without_expansion[-10:]\n",
    "        q = 0.6 * q + 0.5 * np.mean([self.doc_emb[a[0]]for a in near],axis = 0) - 0.1 * np.mean([self.doc_emb[a[0]]for a in far], axis = 0)\n",
    "        return sorted([(key,np.abs(distance.cosine(q,self.doc_emb[key]))) for key in self.doc_emb],key = lambda x : x[1])\n",
    "\n",
    "    def query(self, input_string:str , k, expansion = False) -> List:\n",
    "        word = self.tokenizer(input_string.strip().lower())\n",
    "        matrix = np.array([self.emmbeding[w] for w in word if self.is_c_(w)]).reshape(-1,self.dim)\n",
    "        c = self.c_soft(np.array([self.idf.get(self.mapping.get(w,0),0) for w in word if self.is_c_(w)]).reshape(1,-1))\n",
    "        q = np.matmul(c,matrix)[0]\n",
    "        article_id = self.process_q(q , expansion)[k[0]:k[1]]\n",
    "        articles = [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,articles)\n",
    "\n",
    "source = \"./\"\n",
    "f_source = lambda s : source+\"/\"+s\n",
    "class Transformer:\n",
    "  def __init__(self,docs,model_name = './DATA/sentence-transformers_all-MiniLM-L12-v2/'):\n",
    "    print(f\"Transformer\\ndownloading model {model_name}\")\n",
    "    self.model = SentenceTransformer(model_name)\n",
    "    self.documents = docs\n",
    "    self.representation = None\n",
    "\n",
    "  def preprocess(self,pre_use = False):\n",
    "    if not pre_use:\n",
    "      docs = []\n",
    "      keys = []\n",
    "      print(f\"creating representation for docs\")\n",
    "      for key in self.documents:\n",
    "        abstract = self.documents[key][\"abstract\"]\n",
    "        if type(abstract) == str:\n",
    "          docs.append(abstract)\n",
    "          keys.append(key)\n",
    "      embeddings = self.model.encode(docs)\n",
    "      self.representation = {}\n",
    "      for key, embedding in zip(keys, embeddings):\n",
    "        self.representation[key] = embedding.tolist()\n",
    "      addr = f_source(\"DATA/P3/transformer.json\")\n",
    "      print(f\"saving docs_rep in {addr}\")\n",
    "      open(addr,\"w\").write(json.dumps(self.representation))\n",
    "    print(f\"loading docs_rep\")\n",
    "    self.representation = json.load(open(f_source(\"DATA/P3/transformer.json\"),\"r\"))\n",
    "    self.representation = {key : np.array(self.representation[key]) for key in self.representation }\n",
    "  def query(self,input_str:str , k , expansion = False):\n",
    "    q = self.model.encode(input_str)\n",
    "    without_expansion = sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])\n",
    "    if not expansion:\n",
    "        article_id = without_expansion[k[0]:k[1]]\n",
    "        article =  [self.documents[id[0]] for id in article_id]\n",
    "        return (article_id,article)\n",
    "    near = without_expansion[:10]\n",
    "    far = without_expansion[-10:]\n",
    "    q = 0.6 * q + 0.5 * np.mean([self.representation[a[0]]for a in near]) - 0.1 * np.mean([self.representation[a[0]]for a in far])\n",
    "    return sorted([(key,np.abs(distance.cosine(q,self.representation[key]))) for key in self.representation],key = lambda x : x[1])[k[0]:k[1]]\n",
    "\n",
    "\n",
    "class Page_Ranking_Hits:\n",
    "    def __init__(self):\n",
    "        objective = \"article\"\n",
    "        self.ref_matrix = None\n",
    "        self.articles = sparse.load_npz(\"./DATA/P5/articles_sparse.npz\")\n",
    "        self.objective = self.articles if objective == \"article\" else self.authors\n",
    "        representation = json.load(open(f_source(\"DATA/P5/article_mapping.json\"),\"r\"))\n",
    "        self.mapping = {int(representation[doc]):doc for doc in representation}\n",
    "\n",
    "    def compute_page_rank(self, alpha = 0.9):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.pr = nx.pagerank(G=graph, alpha=alpha)\n",
    "\n",
    "    def compute_hits(self):\n",
    "        graph = nx.from_numpy_array(A=self.objective.toarray(), create_using=nx.DiGraph)\n",
    "        self.hub, self.authority = nx.hits(G=graph)\n",
    "\n",
    "    def tops_pages(self, k = 10):\n",
    "        res = sorted(self.pr.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "        return [self.mapping[int(ar[0])] for ar in res]\n",
    "\n",
    "    def cal_cites(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 0)).reshape(-1)\n",
    "\n",
    "    def top_hubs(self, k = 10):\n",
    "        return sorted(self.hub.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "\n",
    "    def top_auth(self, k = 10):\n",
    "        return sorted(self.authority.items(),key = lambda x : x[1] , reverse = True)[:k]\n",
    "\n",
    "    def cal_ref(self):\n",
    "        return np.asarray(np.sum(self.objective,axis = 1)).reshape(-1)\n",
    "\n",
    "MAIN_DATA_PATH = \"../DATA/clean_data.json\"\n",
    "CLUSTER_DATA_PATH = \"../DATA/clustring_data.csv\"\n",
    "CLASSIFICATION_DATA_PATH = \"../DATA/classification_data.csv\"\n",
    "\n",
    "\n",
    "class IR:\n",
    "    def __init__(self):\n",
    "        print(\"loading requirments ... \")\n",
    "        print(\"loading main data ... \")\n",
    "\n",
    "        self.main_data = json.load(open(address_resolver(MAIN_DATA_PATH),\"r\"))\n",
    "\n",
    "        print(\"loading clustring data ... \")\n",
    "        self.clustring_data = pd.read_csv(address_resolver(CLUSTER_DATA_PATH))\n",
    "        self.cluster_labels_map = {0:\"cs.LG\" , 1:\"cs.CV\" , 2:\"cs.AI\" , 3:\"cs.RO\" , 4:\"cs.CL\"}\n",
    "        self.kmeas_map_label = {0: 2, 1: 2, 2: 1, 3: 3, 4: 2, 5: 1, 6: 1, 7: 4, 8: 1, 9: 1, 10: 1, 11: 1}\n",
    "        self.cluster_model = pickle.load(open(\"DATA/P4/finalized_cluster_model.sav\", 'rb'))\n",
    "        print(\"cluster_model = \",self.cluster_model)\n",
    "\n",
    "        print(\"loading Boolean search model\")\n",
    "        self.boolean_ir = Boolean_IR(self.main_data)\n",
    "        self.boolean_ir.pre_process_authors()\n",
    "        self.boolean_ir.pre_process_title()\n",
    "\n",
    "        print(\"loading tf-idf search model\")\n",
    "        self.tf_idf_raw = TF_IDF_IR(self.main_data)\n",
    "\n",
    "        print(\"loading fasttext module\")\n",
    "        self.fast_text = Fast_text_TF_IDF_IR(self.main_data,t = \"lemma\")\n",
    "        print(\"process fasttext module\")\n",
    "        self.fast_text.preprocess(pre = True ,dim=400, epoch=20 , lr = 0.06 , ws = 10 )\n",
    "\n",
    "        print(\"Transformers loading\")\n",
    "        self.transformer = Transformer(self.main_data,'./DATA/sentence-transformers_all-MiniLM-L12-v2/')\n",
    "        self.transformer.preprocess(pre_use = True)\n",
    "        self.bert_model = self.transformer.model\n",
    "\n",
    "        print(\"page_ranking_algorithm loading\")\n",
    "        self.page_hits_articles = Page_Ranking_Hits()\n",
    "        self.page_hits_articles.compute_page_rank(0.9)\n",
    "        self.page_hits_articles.compute_hits()\n",
    "\n",
    "        print(\"loading classification data ... \")\n",
    "        self.classification_model_name  = 'distilbert-base-uncased'\n",
    "        self.classification_model = AutoModelForSequenceClassification.from_pretrained(\"DATA/P4/classification_model\", from_tf=True)\n",
    "        self.classification_classes = {\n",
    "                        'LABEL_0' : 'cs.CV',\n",
    "                        'LABEL_1' : 'cs.LG',\n",
    "                        'LABEL_2' : 'stat.ML'\n",
    "                    }\n",
    "        self.classification_class_categories = {v: k for k, v in self.classification_classes.items()}\n",
    "        self.classification_tokenizer = AutoTokenizer.from_pretrained(self.classification_model_name)\n",
    "        self.classification_pipeline = TextClassificationPipeline(model=self.classification_model, tokenizer=self.classification_tokenizer, return_all_scores=False)\n",
    "\n",
    "        print(\"Finished loading packages.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def classification(self,text):\n",
    "        prediction = self.classification_pipeline(text)[0]\n",
    "        predicted_class = self.classification_classes[prediction['label']]\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "\n",
    "    def clustring(self,text):\n",
    "        # abstract_bert = bert_model.encode(data_abstract,device = \"cuda\")\n",
    "        # titles_bert = bert_model.encode(data_title,device = \"cuda\")\n",
    "        #concated_data_bert = np.array([np.array([abstract_bert[i],titles_bert[i]]).reshape(-1) for i in range((abstract_bert.shape[0]))])\n",
    "        # text\n",
    "        # class\n",
    "        abstract_bert = self.bert_model.encode([text])\n",
    "        titles_bert = self.bert_model.encode([\"title\"])\n",
    "        concated_data_bert = np.array([np.array([abstract_bert[i],titles_bert[i]]).reshape(-1) for i in range((abstract_bert.shape[0]))])\n",
    "        return self.cluster_labels_map[self.kmeas_map_label[self.cluster_model.predict(concated_data_bert)[0]]]\n",
    "\n",
    "\n",
    "    def search(self,text,type_text,query_expansion = False,mode = \"bert\" , range_q = (0,40)):# abstract title author\n",
    "        #mode = bert , tf-idf , fasttext , boolean\n",
    "        #text_type = abstract , title , author\n",
    "        # range_q = (start , end)\n",
    "        if mode == \"bert\":\n",
    "            return self.transformer.query(text,k = range_q , expansion=query_expansion)\n",
    "        elif mode == \"fasttext\":\n",
    "            return self.fast_text.query(text,range_q,query_expansion)\n",
    "        elif mode == \"tf_idf\":\n",
    "            qt = Query_type.ABSTRACT\n",
    "            if type_text == \"title\":\n",
    "                qt = Query_type.TITLE\n",
    "            return self.tf_idf_raw.query(qt, text , range_q)\n",
    "        elif mode == \"boolean\":\n",
    "            qt = Query_type.TITLE\n",
    "            if type_text == \"author\":\n",
    "                qt = Query_type.AUTHOR\n",
    "            return self.boolean_ir.query(qt,text,range_q)\n",
    "\n",
    "    def best_articles(self,k = 10):# page_rank,hits\n",
    "        return self.page_hits_articles.tops_pages(k = 10)\n",
    "        #self.page_hits_articles.\n",
    "\n",
    "        # type_query = page_rank , hits ,\n",
    "        # k = numbers\n",
    "        # mode = artcile , author\n",
    "\n",
    "        pass\n",
    "ir = IR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ir.search(\"augmentation\" , \"abstract\",True,mode = \"bert\", range_q = (0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
