{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "source = \"../DATA\"\n",
    "from collections import defaultdict\n",
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = json.load(open(f\"{source}/clean_data.json\"))\n",
    "all_artcile_data = lang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paperId': 'bf1bd2dfb4e08868d7e55d1b6fe553761b2299a3', 'title': 'analysis of nonlinear photoconductive (nlp) switching in bulk gaas', 'abstract': 'this paper presents a numerical analysis of nonlinear photoconductive (nlp) switching in bulk gallium arsenide. nlp switches are optically activated switches in which the number of electron-hole pairs initially created are multiplied through a nonlinear process. this process is a combination of double injection, trap filling, and avalanche. nlp switches differ from linear photoconductive (lp) switches in which one photon creates one electron-hole pair. the numerical method used to solve the governing nlp switch equations and preliminary results from the computer model are presented.', 'year': 1991, 'authors': [{'authorId': '31165417', 'name': 'M. Browder'}, {'authorId': '5617715', 'name': 'W. Nunnally'}], 'fieldsOfStudy': ['materials science'], 'citationCount': 6, 'referenceCount': 3, 'references': [{'paperId': '90f811e04ab1df1f635bb511b5c45e00eb9f4eba', 'title': 'picosecond optoelectronic switching and gating in silicon'}, {'paperId': 'ad8d9337a4ceefaf56974cc16885c3b14e79d78f', 'title': 'laser triggered cr:gaas hv sparkgap with high trigger sensitivity'}, {'paperId': 'c88c68595b6adf12e8097c3f7a7191bf4f8b594d', 'title': 'a novel optoelectronic closing and opening switch for pulsed power'}]}\n"
     ]
    }
   ],
   "source": [
    "article = all_artcile_data[\"bf1bd2dfb4e08868d7e55d1b6fe553761b2299a3\"]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543827"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = iter(range(1,len(all_artcile_data)+1));\n",
    "j = iter(range(1,len(all_artcile_data)+1));\n",
    "doc_to_id = defaultdict(lambda : next(i));\n",
    "author_to_id = defaultdict(lambda :[]);\n",
    "list(map(lambda x : doc_to_id[x],all_artcile_data));\n",
    "def put_author(item):\n",
    "    global author_to_id\n",
    "    for author in item:\n",
    "        name = author[\"name\"].lower()\n",
    "        id = author[\"authorId\"]\n",
    "        author_to_id[name] = id\n",
    "        try:\n",
    "            id = int(id)\n",
    "        except:\n",
    "            id = id\n",
    "        author_to_id[id].append(name)\n",
    "        author_to_id[id] = list(set(author_to_id[id]))\n",
    "\n",
    "[put_author(all_artcile_data[item][\"authors\"]) for item in all_artcile_data]\n",
    "assert max(doc_to_id.values()) == len(all_artcile_data) , \"size does not match\"\n",
    "open(\"DATA/P3/doc_to_integerID.json\",\"w\").write(json.dumps(doc_to_id))\n",
    "open(\"DATA/P3/author_to_id.json\",\"w\").write(json.dumps(author_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504850"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_to_doc = defaultdict(lambda :[])\n",
    "for item in all_artcile_data:\n",
    "    ar = all_artcile_data[item]\n",
    "    for author in ar[\"authors\"]:\n",
    "        author_to_doc[author[\"authorId\"]].append(item)\n",
    "open(\"DATA/P3/author_to_doc.json\",\"w\").write(json.dumps(author_to_doc))  \n",
    "i = iter(range(1,len(author_to_id)+1))\n",
    "authorid_to_num_id = defaultdict(lambda : next(i))\n",
    "list(map(lambda x : authorid_to_num_id[x],[key for key in author_to_id if type(key) == int]))\n",
    "open(\"DATA/P3/authorid_to_num_id.json\",\"w\").write(json.dumps(authorid_to_num_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authorid_to_num_id = defaultdict(lambda : next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flatten = lambda x : [item for subitem in x for item in subitem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735170"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "i = 0\n",
    "def nx():\n",
    "    global i\n",
    "    i += 1\n",
    "    return i\n",
    "j = 1\n",
    "def nx2():\n",
    "    global j\n",
    "    j += 1\n",
    "    return j\n",
    "title_lemma = defaultdict(lambda : nx())\n",
    "abstract_lemma = defaultdict(lambda : nx2())\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        for w in nlp(title):\n",
    "            title_lemma[w.lemma_]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in nlp(abstract):\n",
    "            abstract_lemma[w.lemma_]\n",
    "open(\"DATA/P3/title_lemma.json\",\"w\").write(json.dumps(title_lemma))\n",
    "open(\"DATA/P3/abstract_lemma.json\",\"w\").write(json.dumps(abstract_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_dic_title = defaultdict(lambda : [])\n",
    "import numpy as np\n",
    "removed_key = []\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        bool_dic_title[key] = [title_lemma[w.lemma_] for w in nlp(title)]\n",
    "open(\"DATA/P3/bool_dic_title.json\",\"w\").write(json.dumps(bool_dic_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = json.load(open(\"DATA/P3/title_lemma.json\",\"r\"))\n",
    "abs_data = json.load(open(\"DATA/P3/abstract_lemma.json\",\"r\"))\n",
    "import numpy as np\n",
    "freq_matrix_title = np.zeros((len(all_artcile_data)+2,len(title_data)+2))\n",
    "freq_matrix_abstract = np.zeros((len(all_artcile_data)+2,len(abs_data)+2))\n",
    "n = 0\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        for w in nlp(title):\n",
    "            if w.lemma_ not in nlp.Defaults.stop_words:\n",
    "                freq_matrix_title[n][title_data[w.lemma_]] += 1\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in nlp(abstract):\n",
    "            if w.lemma_ not in nlp.Defaults.stop_words:\n",
    "                freq_matrix_abstract[n][abs_data[w.lemma_]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"DATA/P3/matrix.npz\", freq_matrix_title=freq_matrix_title,freq_matrix_abstract=freq_matrix_abstract)\n",
    "v=np.load(\"DATA/P3/matrix.npz\")\n",
    "freq_matrix_title = v[\"freq_matrix_title\"]\n",
    "freq_matrix_abstract = v[\"freq_matrix_abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10750, 9472)\n",
      "(10750, 39056)\n"
     ]
    }
   ],
   "source": [
    "print(freq_matrix_title.shape)\n",
    "print(freq_matrix_abstract.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_title > 0).sum(axis = 0)\n",
    "idf_title = {int(title_data[key]) : (np.log2(M/sum_matrix[title_data[key]]) if sum_matrix[title_data[key]] != 0 else 0) for key in title_data}\n",
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_abstract > 0).sum(axis = 0)\n",
    "idf_abstract = {int(abs_data[key]) : (np.log2(M/sum_matrix[abs_data[key]]) if sum_matrix[abs_data[key]] != 0 else 0) for key in abs_data}\n",
    "open(\"DATA/P3/idf_title.json\",\"w\").write(json.dumps(idf_title))\n",
    "open(\"DATA/P3/idf_abstract.json\",\"w\").write(json.dumps(idf_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10748"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids = [key for key in all_artcile_data]\n",
    "len(article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110726/3948576010.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tf_title = np.log2(1+freq_matrix_title/freq_matrix_title.max(axis=1).reshape(-1,1))\n",
      "/tmp/ipykernel_110726/3948576010.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tf_abs = np.log2(1+freq_matrix_abstract/freq_matrix_abstract.max(axis=1).reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "tf_title = np.log2(1+freq_matrix_title/freq_matrix_title.max(axis=1).reshape(-1,1))\n",
    "tf_abs = np.log2(1+freq_matrix_abstract/freq_matrix_abstract.max(axis=1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "title_tf = {}\n",
    "asb_tf = {}\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"title\"]) == str:\n",
    "        title = article[\"title\"].strip().lower()\n",
    "        title_tf[key] = {title_data[w.lemma_] : tf_title[n][title_data[w.lemma_]] for w in nlp(title) if w.lemma_ not in nlp.Defaults.stop_words}\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        asb_tf[key] = {abs_data[w.lemma_] : tf_abs[n][abs_data[w.lemma_]] for w in nlp(abstract) if w.lemma_ not in nlp.Defaults.stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20456427"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"DATA/P3/title_tf.json\",\"w\").write(json.dumps(title_tf))\n",
    "open(\"DATA/P3/asb_tf.json\",\"w\").write(json.dumps(asb_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flatten = lambda x : [item for subitem in x for item in subitem]\n",
    "not_lemma = \" \".join([all_artcile_data[key][\"abstract\"].lower().strip() for key in all_artcile_data if (type(all_artcile_data[key][\"abstract\"]) == str)])\n",
    "s = \" \".join(flatten([[w.lemma_ for w in nlp(all_artcile_data[key][\"abstract\"].lower().strip())] for key in all_artcile_data if type(all_artcile_data[key][\"abstract\"]) == str]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10493895"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"./fasttext/fasttext_not_lemma_data.txt\",\"w\").write(not_lemma)\n",
    "open(\"./fasttext/fasttext_data.txt\",\"w\").write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061377"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "def nx():\n",
    "    global i\n",
    "    i += 1\n",
    "    return i\n",
    "from nltk.tokenize import word_tokenize\n",
    "abstract_not_lemma = defaultdict(lambda : nx())\n",
    "for key in all_artcile_data:\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in word_tokenize(abstract):\n",
    "            abstract_not_lemma[w]\n",
    "open(\"DATA/P3/abstract_not_lemma.json\",\"w\").write(json.dumps(abstract_not_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1520926"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_data = json.load(open(\"DATA/P3/abstract_not_lemma.json\",\"r\"))\n",
    "import numpy as np\n",
    "freq_matrix_abstract = np.zeros((len(all_artcile_data)+2,len(abs_data)+2))\n",
    "n = 0\n",
    "for key in all_artcile_data:\n",
    "    n += 1\n",
    "    article = all_artcile_data[key]\n",
    "    if type(article[\"abstract\"]) == str:\n",
    "        abstract = article[\"abstract\"].strip().lower()\n",
    "        for w in word_tokenize(abstract):\n",
    "            freq_matrix_abstract[n][abs_data[w]] += 1\n",
    "\n",
    "M = len(all_artcile_data)\n",
    "sum_matrix = (freq_matrix_abstract > 0).sum(axis = 0)\n",
    "idf_abstract = {int(abs_data[key]) : (np.log2(M/sum_matrix[abs_data[key]]) if sum_matrix[abs_data[key]] != 0 else 0) for key in abs_data}\n",
    "open(\"DATA/P3/idf_abstract_not_lemma.json\",\"w\").write(json.dumps(idf_abstract))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
