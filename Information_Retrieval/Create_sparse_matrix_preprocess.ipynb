{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "#765c79514570591907b84b82ebc2766e2c0a0f57\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amirhossein Bagheri 98105621\n",
    "## Mohammad Sadegh majidi Yazdi  98106004\n",
    "## Amirmahdi kousheshi 98171053\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging data\n",
    "\n",
    "<b> skip 3 cell below if you want to use numpy object we already provided and ran this part and saved the output in Module_data you don't have to go throgh whole process it will takes you to downloads 3 Gb data and then needs 15 Gb ram to clean it and create final data.</b>\n",
    "you have to download 4 topics data from [here](https://drive.google.com/drive/folders/19BL85jIEmxre3h8uj6BrFXAn4QoJz6zJ?usp=sharing) and put them in Data folder  it's going to look like this\n",
    "![4 topics](./Module_data/pre_data.jpg)\n",
    "\n",
    "\n",
    "after that run cell below\n",
    "it will need 15 gb ram and will take about 5 minutes to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data\n",
    "merged_data = {}\n",
    "reduced_data = {}\n",
    "f = \"../DATA/clean_data.json\"\n",
    "data = json.load(open(f,\"r\"))\n",
    "for key in data:\n",
    "    merged_data[key] = data[key]\n",
    "    reduced_data[key] = {\"referenceCount\":data[key][\"referenceCount\"],\"citationCount\":data[key][\"citationCount\"],\n",
    "    \"authors\":[l[\"authorId\"] for l in data[key][\"authors\"]],\n",
    "    \"references\":[l[\"paperId\"] for l in data[key][\"references\"]]}\n",
    "del data\n",
    "gc.collect()\n",
    "json.dump(reduced_data,open(\"./DATA/P5/clean_data.json\",\"w\"))\n",
    "print(len(merged_data))\n",
    "del merged_data\n",
    "del reduced_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors = 2780\n",
      "articles = 25331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"./DATA/P5//clean_data.json\",\"r\"))\n",
    "i = 0\n",
    "def count():\n",
    "    global i\n",
    "    j = i\n",
    "    i += 1\n",
    "    return j\n",
    "x = 0\n",
    "def count2():\n",
    "    global x\n",
    "    j = x\n",
    "    x += 1\n",
    "    return j\n",
    "article_mapping = defaultdict(lambda : count())\n",
    "author_mapping = defaultdict(lambda : count2())\n",
    "for index_ar,key in enumerate(data):\n",
    "    if key == None:\n",
    "        continue\n",
    "    article = data[key]\n",
    "    article_mapping[key]\n",
    "    for index,k in enumerate(article[\"authors\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        author_mapping[k]\n",
    "        if index > 5:\n",
    "            break\n",
    "    for index,k in enumerate(article[\"references\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        article_mapping[k]\n",
    "    if index_ar > 1000:\n",
    "        break\n",
    "\n",
    "json.dump(article_mapping,open(\"./DATA/P5/article_mapping.json\",\"w\"))\n",
    "json.dump(author_mapping,open(\"./DATA/P5/author_mapping.json\",\"w\"))\n",
    "print(f\"authors = {len(author_mapping)}\")\n",
    "print(f\"articles = {len(article_mapping)}\")\n",
    "del data\n",
    "del author_mapping\n",
    "del article_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create spare matrix \n",
    "here we create sparse matrix which is graph we need and store it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of authors = 2780\n",
      "total number of edge in authors graph = 0.0\n",
      "total number of articles = 25331\n",
      "total number of edge in authors graph = 10046.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"./DATA/P5/clean_data.json\",\"r\"))\n",
    "author_mapping = json.load(open(\"./DATA/P5/author_mapping.json\",\"r\"))\n",
    "article_mapping = json.load(open(\"./DATA/P5/article_mapping.json\",\"r\"))\n",
    "authors = np.zeros((len(author_mapping),len(author_mapping)))\n",
    "articles = np.zeros((len(article_mapping),len(article_mapping)))\n",
    "for index_ar,key in enumerate(data):\n",
    "    if key == None:\n",
    "        continue\n",
    "    ar_id = article_mapping.get(key,None)\n",
    "    if ar_id == None:\n",
    "        continue\n",
    "    article = data[key]\n",
    "    authors_id_all = []\n",
    "    for index,k in enumerate(article[\"authors\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        aut_id = author_mapping.get(k,None)\n",
    "        if aut_id == None:\n",
    "            continue\n",
    "        authors_id_all.append(aut_id)\n",
    "        if index > 5:\n",
    "            break\n",
    "    for index,k in enumerate(article[\"references\"]):\n",
    "        if k == None:\n",
    "            continue\n",
    "        ref_id = article_mapping.get(k,None)\n",
    "        if ref_id == None:\n",
    "            continue\n",
    "        articles[ar_id][ref_id] += 1\n",
    "        if index > 9:\n",
    "            break\n",
    "    if index_ar > 1000:\n",
    "        break\n",
    "authors = csr_matrix(authors)\n",
    "articles = csr_matrix(articles)\n",
    "print(f\"total number of authors = {len(author_mapping)}\")\n",
    "print(f\"total number of edge in authors graph = {authors.sum()}\")\n",
    "\n",
    "print(f\"total number of articles = {len(article_mapping)}\")\n",
    "print(f\"total number of edge in authors graph = {articles.sum()}\")\n",
    "\n",
    "sparse.save_npz(\"./DATA/P5/authors_sparse.npz\", authors)\n",
    "sparse.save_npz(\"./DATA/P5/articles_sparse.npz\", articles)\n",
    "# your_matrix_back = sparse.load_npz(\"yourmatrix.npz\")\n",
    "del data\n",
    "del author_mapping\n",
    "del article_mapping\n",
    "del articles,authors\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06460c7847243a59d373e01ab79e7881153a1a46f1e755f6579f4f8c151d9554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
